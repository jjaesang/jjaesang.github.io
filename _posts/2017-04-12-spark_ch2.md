---
layout: post
title:  "스파크 맛보기"
date: 2017-04-12 17:04:02
categories: Spark
author : Jaesang Lim
tag: Spark
cover: "/assets/spark.png"
---

# 2. 스파크 맛보기 

1. **스칼라**로 만들어졌음
2. **JVM**위에서 작동 
스파크의 특성이 아닌, 스칼라가 컴파일하면 자바 바이트 코드로 변환
Java 6 이상 / Python 2.6 이상

## 스파크 다운로드 하기 

하둡 클러스터나 HDFS를 사용중이라면, 하둡 버전에 맞게 다운로드하자!
[http://spark.apache.org/downloads.html](http://) 클릭!

스파크를 설치를 했다면 아래와 같은 파일들이 있다. ( 버전마다 조금 다른듯하다.. )
1. README.md
 짧은 소개

2. bin
스파크 쉘 등등
3. core , streaming, python
주요 컴포넌트들의 소스코드
4. example

스파크를 설치를 마쳤다면.. 이제 스파크를 실행시킬 수 있는 클러스터 매니저(clusterManager )을 알아야한다. 
1. local 모드 ( = 스파크 배포판에 포함된 단독 스케줄러)
2. 메소스(Mesos)
> - 데이터 센터 내의 자원을 공유/격리를 관리하는 기술로 개발된 Mesos
> - 응용 프로그램과 풀링된 서버 간의 추출 레이어로 작동되며, 분산 환경에서 작업 실행을 최적화
> - Hadoop, MPI, Hypertable, Spark 같은 응용 프로그램을 동적 클러스터 환경에서 리소스 공유와 분리를 통해 자원 최적화가 가능
> - 즉, Mesos는 클러스터에서 사용 가능한 계산 자원을 추적하고 사용자의 요구에 따라 그것을 할당하는 일을 한다
> - Mesos의 매커니즘은 클러스터링 환경에서 동적으로 자원을 할당하고 격리해 주는 매커니즘을 가지고 있어 장점으로 활용
> - 이는 응용 프로그램에 맞는 최적의 자원을 할당(공유된 기존 자원을 적절하게 할당)해서 돌아가기 때문에 응용 프로그램 간의 자원 간섭을 Mesos가 막아줘(격리) 독립적으로 응용프로램이 해당 자원을 잘 활용해서 실행을 완료
 
3. 얀 ( Yarn )
> -YARN은 맵리듀스와 다른 프레임워크에 대한 스케줄링과 오류 모니터링, 그리고 데이터 로컬리티 등과 같은 서비스를 제공하는 자원 관리 플랫폼





 
## 스파크의 파이썬 셸과 스칼라 쎌 소개

- 스파크는 즉석 데이터 분석이 가능하도록 `대화형 셸` 제공을 제공한다. 
- bash shell 과의 다른 점이라고 한다면 `여러 머신의 memory나 disk에 분산되어 존재하는 데이터들과 통신할 수 있다는 것`이다.
- 즉, 분산처리가 가능하다.

*(스파크 애플리케이션을 개발하기 전이나, 개발한 후에 테스트에 활용될 것 같다 ... 개인적인 생각)*

- 그렇다면.. 분산처리가 가능하다는 것에 대해 생각해보자.
- 분산 시스템은 일반적으로 클러스터 내의 다수의 노드, 즉 머신(=서버)에서 병렬적으로 처리한다. 
- 스파크도 마찬가지로 작업 노드( 하둡에서의 dataNode)에서 데이터를 Memory에 올려 분산 작업처리 가능하다.
- 연산 과정을 클러스터 전체에 걸쳐 자동으로 병렬화해 분산 배치된 연산 작업들의 모음으로 표현된다.

* 분산 배치된 연산 작업들의 모음? 이게 무슨말인가?..
> 연산 모음이란 **RDD** = 분산데이터와 연산을 위한 스파크 핵심
[Resilient Distributed Datasets (RDD) 논문 정리](https://www.slideshare.net/yongho/rdd-paper-review)

## 스파크의 핵심 개념 소개
[Spark의 핵심 요소들 - Cluster](https://www.slideshare.net/HyeonSeokChoi/cluster-spark?qid=ddfff5c9-4e43-4d93-b7c2-c7314e35b5ac&v=&b=&from_search=1 )

![Spark master/slave architecture](https://i.stack.imgur.com/cwrMN.png)

- 위의 그림... 끝이다. 정말 간단하다
- 모든 스파크 애플리케이션은 클러스터에서 다양한 병렬 연산을 수행하는 **Driver 프로그램** 
- Driver 프로그램과 연산 클러스터에 대한 연결을 나타내는 객체 , 한마디로 개별 Task를 실행하는 프로세스인 **Executor**
- Driver 프로그램과 Excutor를 실행시실 수 있는 **Cluster Manager** 로 구성되어 있다.

* driver 와 executor 가 어떻게 최초 실행이 될까? 
- Spark 는 Cluster Manager 를 통해서 executor 와 driver(특정 상황에서) 를 실행한다.
- Cluster Manager 는 다른 컴포넌트로 교체가 가능한데, YARN 이나 Mesos 같은 외부 Cluster Manager 에서나 내부 standalone cluster manager 로 실행이 가능하다


**Driver** 
--- 
- main() 메소드가 실행되는 프로세스이다. 
- Driver 프로그램은 사용자의 프로그램을 Task로 변환하여 Cluster에 전송한다.
- 스파크 애플리케이션들은 RDD 연산으로 구성된다. 
- Driver 프로그램은 연산들의 관계를 DAG ( Directed Acyclic Gragh)로 생성하고, 이 DAG를 물리적인 실행 계획으로 변환한다. 

*( Apache Pig의 pig Latin를 논리적 계획으로 변환 후 물리적 실행 계획으로 옮기는 것과 유사한 것으로 보인다. )*

- DAG? 방향성이 있지만 Cycle은 존재하지 않는 그래프 구조로 '방향성 비순환 그래프'이다.
- 이 실행 계획을 최적화를 거쳐 여러개의 stage로 변환하고, 각 stage는 여러개의 task로 구성된다. 
- 일련의 작업이 끝난 뒤, 단위 작업을 묶어서 Cluster로 전송한다. 

- Executor들은 시작시 Driver에 등록되고, Driver은 계속 Executor들을 감시한다.
- Task를 데이터 위치에 기반해 적절한 위치에서 실행되로록한다. ( = Locality를 보장한다 )

**SparkContext ( sc )**
---
- Driver 프로그램과 연산 클러스터에 대한 연결을 나타내는 Object

**Executor**
---
- 개별 Task를 실행하는 프로세스라고 설명했다. 
- Task 실행 후 결과를 Driver에 전송하며, 사용자 프로그램에서 캐싱한 RDD를 저장하기 위한 메모리 공간 제공한다. 

**Cluster Manager**
---
- Spark는 Executor를 실행하기 위해 ClusterManager에 의존한다
- 종류로는 Standalone / Hadoop Yarn / Apache Mesos 가 있다. 

## 단독 애플리케이션 

- 셸과 의 가장 큰 차이는 ‘SparkContext’ 객체를 초기화해야한다는 것이다.
- 즉, Spark 관련 패키지 Import하여 SparkContext를 사용해야하며, SparkConf Object를 통해 SparkContext Object를 생성한다.  

``` scala
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

val conf = new SparkConf().setMaster(“local”). setAppName(“Spark SCALA”)
val sc = new SparkContext(conf)

sc.stop()       :  Spark ShutDown
setMaster()     :  접속할 Cluster URL 입력
setAppName()    :  Cluster UI에서 구분할 이름  


```
```` java
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;

SparkConf conf = new SparkConf().setMaster(“local”).setAppName(“Spark JAVA”);
JavaSparkContext sc = new JavaSparkContext(conf);

System.exit(0)
sys.exit()

````
