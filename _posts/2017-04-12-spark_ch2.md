---
layout: post
title:  "스파크 맛보기"
date: 2017-04-12 17:04:02
categories: Spark
tag: Spark
cover: "/assets/instacode.png"
---

# 2. 스파크 맛보기 

1. **스칼라**로 만들어졌음
2. **JVM**위에서 작동 
스파크의 특성이 아닌, 스칼라가 컴파일하면 자바 바이트 코드로 변환
Java 6 이상 / Python 2.6 이상

## 스파크 다운로드 하기 

하둡 클러스터나 HDFS를 사용중이라면, 하둡 버전에 맞게 다운로드하자!
[http://spark.apache.org/downloads.html](http://) 클릭!

스파크를 설치를 했다면 아래와 같은 파일들이 있다. ( 버전마다 조금 다른듯하다.. )
1. README.md
 짧은 소개

2. bin
스파크 쉘 등등
3. core , streaming, python
주요 컴포넌트들의 소스코드
4. example

스파크를 설치를 마쳤다면.. 이제 스파크를 실행시킬 수 있는 클러스터 매니저(clusterManager )을 알아야한다. 
1. local 모드 ( = 스파크 배포판에 포함된 단독 스케줄러)
2. 메소스(Mesos)
>데이터 센터 내의 자원을 공유/격리를 관리하는 기술로 개발된 Mesos
응용 프로그램과 풀링된 서버 간의 추출 레이어로 작동되며, 분산 환경에서 작업 실행을 최적화
Hadoop, MPI, Hypertable, Spark 같은 응용 프로그램을 동적 클러스터 환경에서 리소스 공유와 분리를 통해 자원 최적화가 가능
즉, Mesos는 클러스터에서 사용 가능한 계산 자원을 추적하고 사용자의 요구에 따라 그것을 할당하는 일을 한다
Mesos의 매커니즘은 클러스터링 환경에서 동적으로 자원을 할당하고 격리해 주는 매커니즘을 가지고 있어 장점으로 활용
이는 응용 프로그램에 맞는 최적의 자원을 할당(공유된 기존 자원을 적절하게 할당)해서 돌아가기 때문에 응용 프로그램 간의 자원 간섭을 Mesos가 막아줘(격리) 독립적으로 응용프로램이 해당 자원을 잘 활용해서 실행을 완료
 
3. 얀 ( Yarn )
>YARN은 맵리듀스와 다른 프레임워크에 대한 스케줄링과 오류 모니터링, 그리고 데이터 로컬리티 등과 같은 서비스를 제공하는 자원 관리 플랫폼





 
## 스파크의 파이썬 셸과 스칼라 쎌 소개

스파크는 즉석 데이터 분석이 가능하도록 `대화형 셸` 제공을 제공한다. 
bash shell 과의 다른 점이라고 한다면 `여러 머신의 memory나 disk에 분산되어 존재하는 데이터들과 통신할 수 있다는 것`이다. 즉, 분산처리가 가능하다.

*(스파크 애플리케이션을 개발하기 전이나, 개발한 후에 테스트에 활용될 것 같다 ... 개인적인 생각)*

그렇다면.. 분산처리가 가능하다는 것에 대해 생각해보자.
분산 시스템은 일반적으로 클러스터 내의 다수의 노드, 즉 머신(=서버)에서 병렬적으로 처리한다. 
스파크도 마찬가지로 작업 노드( 하둡에서의 dataNode)에서 데이터를 Memory에 올려 분산 작업처리 가능하고, 연산 과정을 클러스터 전체에 걸쳐 자동으로 병렬화해 분산 배치된 연산 작업들의 모음으로 표현된다.

분산 배치된 연산 작업들의 모음? 이게 무슨말인가?..
연산 모음이란 **RDD** = 분산데이터와 연산을 위한 스파크 핵심
[Resilient Distributed Datasets (RDD) 논문 정리](https://www.slideshare.net/yongho/rdd-paper-review)

## 스파크의 핵심 개념 소개
[Spark의 핵심 요소들 - Cluster](https://www.slideshare.net/HyeonSeokChoi/cluster-spark?qid=ddfff5c9-4e43-4d93-b7c2-c7314e35b5ac&v=&b=&from_search=1 )

![Spark master/slave architecture](https://i.stack.imgur.com/cwrMN.png)

위의 그림... 끝이다. 정말 간단하다
모든 스파크 애플리케이션은 클러스터에서 다양한 병렬 연산을 수행하는 **Driver 프로그램** 
Driver 프로그램과 연산 클러스터에 대한 연결을 나타내는 객체 , 한마디로 개별 Task를 실행하는 프로세스인 **Executor**
Driver 프로그램과 Excutor를 실행시실 수 있는 **Cluster Manager** 로 구성되어 있다.

driver 와 executor 가 어떻게 최초 실행이 될까? 
Spark 는 Cluster Manager 를 통해서 executor 와 driver(특정 상황에서) 를 실행한다. 이런 Cluster Manager 는 다른 컴포넌트로 교체가 가능한데, YARN 이나 Mesos 같은 외부 Cluster Manager 에서나 내부 standalone cluster manager 로 실행이 가능하다


**Driver** 
>main() 메소드가 실행되는 프로세스이다. 
Driver 프로그램은 사용자의 프로그램을 Task로 변환하여 Cluster에 전송한다.
Task로 변환?..
스파크 애플리케이션들은 RDD 연산으로 구성된다. 
Driver 프로그램은 연산들의 관계를 DAG ( Directed Acyclic Gragh)로 생성하고, 이 DAG를 물리적인 실행 계획으로 변환한다. 
*( Apache Pig의 pig Latin를 논리적 계획으로 변환 후 물리적 실행 계획으로 옮기는 것과 유사한 것으로 보인다. )*
DAG? 방향성이 있지만 Cycle은 존재하지 않는 그래프 구조로 '방향성 비순환 그래프'이다.
이 실행 계획을 최적화를 거쳐 여러개의 stage로 변환하고, 각 stage는 여러개의 task로 구성된다. 일련의 작업이 끝난 뒤, 단위 작업을 묶어서 Cluster로 전송한다. 

Executor들은 시작시 Driver에 등록되고, Driver은 항상~ Executor들을 감시한다.
Task를 데이터 위치에 기반해 적절한 위치에서 실행되로록한다. ( Locality를 보장한다 )

**SparkContext ( sc )**
>Driver 프로그램과 연산 클러스터에 대한 연결을 나타내는 Object

**Executor**
>개별 Task를 실행하는 프로세스라고 설명했다. Task 실행 후 결과를 Driver에 전송하며
사용자 프로그램에서 캐싱한 RDD를 저장하기 위한 메모리 공간 제공한다. 

**Cluster Manager**
>Spark는 Executor를 실행하기 위해 ClusterManager에 의존하며 종류로는 Standalone / Hadoop Yarn / Apache Mesos 가 있다. 

## 단독 애플리케이션 

셸과 의 가장 큰 차이는 ‘SparkContext’ 객체를 초기화해야한다는 것이다.
즉, Spark 관련 패키지 Import하여 SparkContext를 사용해야하며, SparkConf Object를 통해 SparkContext Object를 생성한다.  

``` scala
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

val conf = new SparkConf().setMaster(“local”). setAppName(“Spark SCALA”)
val sc = new SparkContext(conf)

sc.stop()       :  Spark ShutDown
setMaster()     :  접속할 Cluster URL 입력
setAppName()    :  Cluster UI에서 구분할 이름  


```
```` java
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;

SparkConf conf = new SparkConf().setMaster(“local”).setAppName(“Spark JAVA”);
JavaSparkContext sc = new JavaSparkContext(conf);

System.exit(0)
sys.exit()

````
